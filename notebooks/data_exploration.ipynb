{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_chapters = [\n",
    "    [0, 1, 5, 6, 10, 11, 12, 13, 14, 15],\n",
    "    [3, 4, 8, 9, 25, 26],\n",
    "    [7, 16, 27, 28, 29, 20, 21, 22, 23, 24]\n",
    "]\n",
    "\n",
    "agent_system_prompts = [\n",
    "    \"\"\"You are a specialized assistant, focused exclusively on providing expert answers about General Vehicle Registration and Licensing.\n",
    "You offer precise, detailed guidance on topics like general registration information, licensee requirements, odometer mileage reporting, \n",
    "and the sale of new vehicles by California dealers. You specialize in helping with registration renewals, ownership transfers, commercial\n",
    "vehicle regulations, nonresident vehicle registration, and the Permanent Trailer Identification (PTI) program. Your expertise also extends\n",
    "to off-highway vehicles and all other specific registration-related inquiries.\"\"\",\n",
    "    \"\"\" \"\"\",\n",
    "    \"\"\" \"\"\",\n",
    "]\n",
    "\n",
    "DATASET_PATH = \"../qa_pairs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_qa_json(json_path):\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_qa_chapter(chapters_list):\n",
    "    qa_count = 0\n",
    "    for chapter in chapters_list:\n",
    "        chapter_path = os.path.join(DATASET_PATH, f\"ch{chapter:02d}\")\n",
    "        for filename in os.listdir(chapter_path):\n",
    "            if filename.endswith('.json'):\n",
    "                file_path = os.path.join(chapter_path, filename)\n",
    "                qa_count += count_qa_json(file_path)\n",
    "    return qa_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0, qa count: 2157\n",
      "Agent 1, qa count: 864\n",
      "Agent 2, qa count: 1745\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(agent_chapters)):\n",
    "    print(f\"Agent {i}, qa count:\", count_qa_chapter(agent_chapters[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gpt finetune api expects:\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, \n",
    "{\"role\": \"user\", \"content\": \"What's the capital of France?\"}, \n",
    "{\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]}\n",
    "\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(arr, test_size=0.2, shuffle=True):\n",
    "    data = arr.copy()\n",
    "    \n",
    "    if shuffle:\n",
    "        random.shuffle(data)\n",
    "    \n",
    "    split_index = int(len(data) * (1 - test_size))\n",
    "    \n",
    "    train_data = data[:split_index]\n",
    "    test_data = data[split_index:]\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffled_chapter_qa(chapters_list, system_prompt, shuffle=True, save=False, savepath=None):\n",
    "    all_chapter_questions = []\n",
    "    for chapter in chapters_list:\n",
    "        chapter_path = os.path.join(DATASET_PATH, f\"ch{chapter:02d}\")\n",
    "        for filename in os.listdir(chapter_path):\n",
    "            if filename.endswith('.json'):\n",
    "                file_path = os.path.join(chapter_path, filename)\n",
    "                with open(file_path, 'r') as file:\n",
    "                    data = json.load(file)\n",
    "                    all_chapter_questions.extend(data)\n",
    "\n",
    "        gpt_formatqa = list(map(lambda qapair: {\"messages\":    \n",
    "            [{\"role\": \"system\", \"content\": system_prompt}] \n",
    "            + qapair\n",
    "            },\n",
    "            all_chapter_questions\n",
    "        ))\n",
    "    \n",
    "    return train_test_split(gpt_formatqa, test_size=0.2, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created data for agent00\n",
      "Train samples: 1725\n",
      "Test samples: 432\n",
      "\n",
      "Created data for agent01\n",
      "Train samples: 691\n",
      "Test samples: 173\n",
      "\n",
      "Created data for agent02\n",
      "Train samples: 1396\n",
      "Test samples: 349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FINETUNE_AGENT_DATADIR = \"../openai_finetune_data\"\n",
    "for i in range(len(agent_chapters)):\n",
    "    train, test = shuffled_chapter_qa(agent_chapters[i], agent_system_prompts[i])\n",
    "\n",
    "    agent_dir = os.path.join(FINETUNE_AGENT_DATADIR, f\"agent{i:02d}\")\n",
    "    os.makedirs(agent_dir, exist_ok=True)\n",
    "    \n",
    "\n",
    "    train_file = os.path.join(agent_dir, \"train.jsonl\")\n",
    "    with open(train_file, 'w') as f:\n",
    "        for item in train:\n",
    "            json.dump(item, f)\n",
    "            f.write('\\n')\n",
    "    \n",
    "    test_file = os.path.join(agent_dir, \"test.jsonl\")\n",
    "    with open(test_file, 'w') as f:\n",
    "        for item in test:\n",
    "            json.dump(item, f)\n",
    "            f.write('\\n')\n",
    "    \n",
    "    print(f\"Created data for agent{i:02d}\")\n",
    "    print(f\"Train samples: {len(train)}\")\n",
    "    print(f\"Test samples: {len(test)}\")\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
